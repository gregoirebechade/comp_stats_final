{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont 36 eeg avec certains egge \"anormaux\" et d'autres \"normaux\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data from https://www.kaggle.com/datasets/amananandrai/complete-eeg-dataset/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "durée = 60s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "freq = 516 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=np.array([0,1,1,1,0,1,0,1,1,0,0,1,1,1,0,1,1,1,1,0,1,0,0,1,1,1,1,1,1,1,0,1,1,1,1,1])\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ce que je fais c'est que je découpe chaque eeg en petits patchs de 1000 points. La tache de classification sera de prendre un patch de 1000 points et de le classifier en malade / non malade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set construct_files to True if you want to construct the files\n",
    "construct_files = False\n",
    "if construct_files : \n",
    "    np.random.seed(0)\n",
    "    counts = [str(i) for i in range(31)]\n",
    "    counts = ['0' + i if len(i) == 1 else i for i in counts]\n",
    "\n",
    "    for file_name in counts : \n",
    "        df = pd.read_csv('./../data/kaggle_2/s'+file_name+'.csv', header=None).transpose().to_numpy()\n",
    "        for i in range(31*4): \n",
    "            sample = pd.DataFrame(df[:, 250*i:250*i+1000]) # slicing of 250 \n",
    "            random_float = np.random.rand()\n",
    "            if random_float < 0.1:\n",
    "                sample.to_csv('./../data/test/'+file_name+'_'+str(i)+'_'+'.csv', index=False, header=False)\n",
    "            elif random_float < 0.3 : \n",
    "                sample.to_csv('./../data/validation/'+file_name+'_'+str(i)+'_'+'.csv', index=False, header=False)\n",
    "            else:\n",
    "                sample.to_csv('./../data/train/'+file_name+'_'+str(i)+'_'+'.csv', index=False, header=False)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la cellule suivante, je définis un dataset pretraining. La tache de pretraining consiste à prendre deux patchs de longueur 1000 dans un même eeg et de dire s'ils sont à côté ou non. \n",
    "\n",
    "Ainsi, on découpe chaque eeg en patchs glissants de taille 1000 et glissant de 250 à chaque fois. Ca fait 31*4 - 4 = 120 patchs. __getitem__ renvoie soit un patch et son voisin de droite à 500 points, soit un patch et le patch situé à 15500 points de lui (on rapelle que les eeg font 31 000 points). En plus de cela, il renvoie les indices des deux patchs (pour la classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_pretraining(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_data, n_files=36, n_samples_per_file=31*4, segment_length=1000, slide = 250):\n",
    "        self.path_to_data = path_to_data\n",
    "        self.n_files = n_files\n",
    "        self.slide = slide\n",
    "        self.n_samples_per_file = n_samples_per_file\n",
    "        self.segment_length = segment_length\n",
    "        self.data = []\n",
    "        for file in range(self.n_files):\n",
    "            x = pd.read_csv(self.path_to_data + 's' + str(file).zfill(2) + '.csv', header=None).transpose().to_numpy()\n",
    "            self.data.append(x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_files * self.n_samples_per_file*2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = idx // ((31*4)*2)\n",
    "        sample = (idx % ((31*4)*2))\n",
    "        first = (sample % (31*4))*250\n",
    "        if first+1000 >= 31000:\n",
    "            first = 30000-1\n",
    "        if sample // (31*4) == 0:  # proches, second est une fenêtre proche de first\n",
    "            if first + 1500 > 31000:\n",
    "                second  = first  - 500\n",
    "            else : \n",
    "                second = first + 500\n",
    "        else: # éloignés, second est une fenêtre éloignée de first\n",
    "            assert sample // (31*4) == 1\n",
    "            second = first + 15500\n",
    "            if second + 1000 > 31000:\n",
    "                second = 15500\n",
    "        x1 = self.data[file][:, first: first+self.segment_length]  # Utilisation de la donnée préchargée\n",
    "        x2 = self.data[file][:, second: second+self.segment_length]\n",
    "        return torch.stack([torch.tensor(x1), torch.tensor(x2)]), torch.tensor([first, second])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la cellule suivante, on définit le dataset pour la tache finale de classification. On renvoie un sous_eeg de 1000 points ainsi que le label auquel il est associé. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_data):\n",
    "        self.path_to_data = path_to_data # par example './../data/train/\n",
    "        self.X = os.listdir(self.path_to_data) # the list of the files in the train set \n",
    "        self.data=[]\n",
    "        self.labels=np.array([0,1,1,1,0,1,0,1,1,0,0,1,1,1,0,1,1,1,1,0,1,0,0,1,1,1,1,1,1,1,0,1,1,1,1,1])\n",
    "        for idx in range(len(self.X)):\n",
    "            x = pd.read_csv(self.path_to_data + self.X[idx], header=None).to_numpy()\n",
    "            self.data.append((x, self.labels[int(self.X[idx].split('_')[0])]))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x , y = self.data[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell to build the dataloaders. J'ai fait n'imp sur les dataloaders train, val et test donc ça prend un temps fou à charger; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train finished\n",
      "val finished\n",
      "test finished\n"
     ]
    }
   ],
   "source": [
    "dataloader_train = DataLoader(Mydataset('./../data/train/'), batch_size=5, shuffle=True)\n",
    "print('train finished')\n",
    "dataloader_val = DataLoader(Mydataset('./../data/validation/'), batch_size=5, shuffle=True)\n",
    "print('val finished')\n",
    "dataloader_test = DataLoader(Mydataset('./../data/test/'), batch_size=5, shuffle=True)\n",
    "print('test finished')\n",
    "dataloader_pretraining = DataLoader(Dataset_pretraining('./../data/kaggle_2/'), batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 19, 1000])\n",
      "torch.Size([1, 2])\n",
      "--------------------\n",
      "torch.Size([5, 19, 1000])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader_pretraining :\n",
    "  print(batch[0].shape)\n",
    "  print(batch[1].shape)\n",
    "  break\n",
    "print('--------------------')\n",
    "for batch in dataloader_train :\n",
    "  print(batch[0].shape)\n",
    "  print(batch[1].shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La c'est notre feature extractor, convolutionnel grosso modo comme ce qu'on a dans l'article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGFeatureExtractor(nn.Module):\n",
    "    def __init__(self, feature_dim=100):\n",
    "        super(EEGFeatureExtractor, self).__init__()\n",
    "        # input [batch_size, 19, 1000]\n",
    "        self.conv1= nn.Conv1d(19, 32, 3, padding=1)\n",
    "        self.conv2= nn.Conv1d(32, 64, 3, padding=1)\n",
    "        self.conv3= nn.Conv1d(64, 128, 3, padding=1)\n",
    "        self.conv4= nn.Conv1d(6, 10, 3, padding=1)   \n",
    "        self.conv5 = nn.Conv1d(64, 15, 3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(1, 13)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(150, 100)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('au début', x.shape)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        # print(1, x.shape)\n",
    "        x = self.pool(x)\n",
    "        # print(2, x.shape)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        # print(3, x.shape)\n",
    "        x = self.pool(x)\n",
    "        # print(4, x.shape)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        # print(5, x.shape)\n",
    "        x = self.relu(self.conv4(x))\n",
    "        # print(6, x.shape)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        # print(7, x.shape)\n",
    "        x = self.conv5(x)\n",
    "        # print(8, x.shape)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        # print(9, x.shape)\n",
    "        x = self.dropout(x)\n",
    "        # print('a la fin', x.shape)\n",
    "        x = self.fc(x)\n",
    "        return self.relu(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 19, 1000])\n",
      "torch.Size([5, 100])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader_train :\n",
    "  x = batch[0]\n",
    "  print(x.shape)\n",
    "  model = EEGFeatureExtractor()\n",
    "  print(model(x.float()).shape)  \n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraine le features extractor : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin_vers_sauvegarde = './models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grego\\miniconda3\\envs\\map588\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "train_extractor = False\n",
    "tau = 516 # 1 seconde\n",
    "model_name='extractor'\n",
    "if not os.path.exists('./models/'+model_name):\n",
    "    os.makedirs('./models/'+model_name)\n",
    "device = 'cpu'\n",
    "model = EEGFeatureExtractor()\n",
    "n_epochs=200\n",
    "loss = torch.nn.L1Loss()\n",
    "param_1 = torch.nn.Parameter(torch.ones(100, requires_grad=True))\n",
    "param_2 =  torch.nn.Parameter(torch.ones(1, requires_grad=True))\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params': model.parameters()}, {'params': [param_1, param_2]}],\n",
    "    lr=0.1\n",
    ")\n",
    "model.to(device)\n",
    "loss_train=[]\n",
    "if train_extractor:\n",
    "    for epoch in (range(n_epochs)):\n",
    "        print('epoch', epoch)\n",
    "        losstrain=0\n",
    "        counttrain=0\n",
    "        lossval=0\n",
    "        countval=0\n",
    "        for batch_x,batch_y in dataloader_pretraining:\n",
    "            batch_x=batch_x[0].to(device)\n",
    "            batch_y = batch_y.long()\n",
    "            batch_y=batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            first_window = batch_x[0]\n",
    "            second_window = batch_x[1]\n",
    "            # print('the shape is', first_window.float().shape)\n",
    "            first_prediction = model(first_window.float().unsqueeze(0))\n",
    "            second_prediction = model(second_window.float().unsqueeze(0))\n",
    "            label_predicted = torch.dot(param_1, abs(first_prediction - second_prediction).squeeze()) + param_2\n",
    "            idx_1 = batch_y[0][0]\n",
    "            idx_2 = batch_y[0][1]\n",
    "            if (\n",
    "                \n",
    "                abs(idx_1- idx_2 ) < 1000 # close in time\n",
    "            ) : \n",
    "                y_pred = torch.tensor([-1]).to(device)\n",
    "            else:\n",
    "                y_pred = torch.tensor([1]).to(device) # 1 s'ils sont proches, -1 sinon\n",
    "            l=-torch.nn.functional.logsigmoid(y_pred * label_predicted)\n",
    "            # l=torch.log(1+torch.exp(-y_pred*label_predicted))\n",
    "            counttrain+=1\n",
    "            l.backward()\n",
    "            losstrain+=l\n",
    "            optimizer.step()\n",
    "        if epoch%10==0:\n",
    "            print(f'epoch {epoch}, training loss = {losstrain/counttrain}')\n",
    "        loss_train.append(losstrain/counttrain)\n",
    "        \n",
    "    torch.save(model, chemin_vers_sauvegarde+'_final'+'.pth')\n",
    "\n",
    "\n",
    "    # saving the losses in txt files : \n",
    "    loss_list_train = [loss_train[i].detach().cpu().numpy() for i in range(len(loss_train))]\n",
    "\n",
    "\n",
    "\n",
    "    with open('./losses/loss_train_'+model_name+'.txt', 'w') as f :\n",
    "        for elt in loss_list_train : \n",
    "            f.write(str(elt) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Là hop le classifieur, qui est constitué d'un feature extractor puis d'une couche fully connected. Soit on entraine tout d'un coup soit on entraine séparément les deux parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from train_classifiers import EEGClassifier\n",
    "# class EEGClassifier(nn.Module):\n",
    "#     def __init__(self, feature_extractor):\n",
    "#         super(EEGClassifier, self).__init__()\n",
    "#         self.feature_extractor = feature_extractor\n",
    "#         self.fc = nn.Linear(100, 1)\n",
    "#         self.f = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         features = self.feature_extractor(x)\n",
    "#         features = F.normalize(features, p=2, dim=1)\n",
    "#         x = self.fc(features)\n",
    "#         return self.f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La c'est les boucles d'entrainement qui sont aussi dans le fichier .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = torch.load('./models/extractor.pth')\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "model = EEGClassifier(pretrained)\n",
    "# loss for classification : \n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "model_name = 'classifier_pretrained'\n",
    "loss_train=[]\n",
    "n_epochs=200\n",
    "optimizer = torch.optim.Adam(model.fc.parameters())\n",
    "for epoch in range(n_epochs):\n",
    "    print('epoch', epoch)\n",
    "    losstrain=0\n",
    "    counttrain=0\n",
    "    for batch_x,batch_y in dataloader_train:\n",
    "        batch_x=batch_x.to(device)\n",
    "        batch_y = batch_y.float()\n",
    "        batch_y=batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_x.float())\n",
    "        l=loss(y_pred.squeeze(), batch_y)\n",
    "        counttrain+=1\n",
    "        l.backward()\n",
    "        losstrain+=l\n",
    "        optimizer.step()\n",
    "    if epoch%10==0:\n",
    "        print(f'epoch {epoch}, training loss = {losstrain/counttrain}')\n",
    "    loss_train.append(losstrain/counttrain)\n",
    "torch.save(model, chemin_vers_sauvegarde+model_name+'_final'+'.pth')\n",
    "loss_list_train = [loss_train[i].detach().cpu().numpy() for i in range(len(loss_train))]\n",
    "with open('./losses/loss_train_'+model_name+'.txt', 'w') as f :\n",
    "    for elt in loss_list_train : \n",
    "        f.write(str(elt) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_pretrained = EEGFeatureExtractor()\n",
    "model = EEGClassifier(not_pretrained)\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "n_epochs=200\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "model_name = 'classifier_not_pretrained'\n",
    "loss_train=[]\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "n_epochs=200\n",
    "# optimizer = torch.optim.Adam(model.fc.parameters())\n",
    "for epoch in range(n_epochs):\n",
    "    print('epoch', epoch)\n",
    "    losstrain=0\n",
    "    counttrain=0\n",
    "    for batch_x,batch_y in dataloader_train:\n",
    "        batch_x=batch_x.to(device)\n",
    "        batch_y = batch_y.float()\n",
    "        batch_y=batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_x.float())\n",
    "        l=loss(y_pred.squeeze(), batch_y)\n",
    "        counttrain+=1\n",
    "        l.backward()\n",
    "        losstrain+=l\n",
    "        optimizer.step()\n",
    "    if epoch%10==0:\n",
    "        print(f'epoch {epoch}, training loss = {losstrain/counttrain}')\n",
    "    loss_train.append(losstrain/counttrain)\n",
    "torch.save(model, chemin_vers_sauvegarde+'_final'+'.pth')\n",
    "loss_list_train = [loss_train[i].detach().cpu().numpy() for i in range(len(loss_train))]\n",
    "with open('./losses/loss_train_'+model_name+'.txt', 'w') as f :\n",
    "    for elt in loss_list_train : \n",
    "        f.write(str(elt) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dernière cellule pour l'évaluation finale !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7683],\n",
      "        [0.6948],\n",
      "        [0.7633],\n",
      "        [0.7547],\n",
      "        [0.7415]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('./models/classifier_pretrained_final_bis.pth')\n",
    "model_not_pretrained = torch.load('./models/classifier_not_pretrained.pth')\n",
    "for batch_x, batch_y in dataloader_test : \n",
    "    batch_x = batch_x.to(device)\n",
    "    batch_y = batch_y.float()\n",
    "    batch_y = batch_y.to(device)\n",
    "    y_pred = model(batch_x.float())\n",
    "    print(y_pred)\n",
    "    # print(model_not_pretrained(batch_x.float()))\n",
    "    print(batch_y)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EEGClassifier' object has no attribute 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[188], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m y_pretrained \u001b[38;5;241m=\u001b[39m pretrained(batch_x\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m---> 11\u001b[0m y_not_pretrained \u001b[38;5;241m=\u001b[39m \u001b[43mnot_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m y_true\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mconcatenate((y_true, batch_y\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[0;32m     13\u001b[0m y_pretrained \u001b[38;5;241m=\u001b[39m y_pretrained\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\grego\\miniconda3\\envs\\map588\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\grego\\miniconda3\\envs\\map588\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[163], line 12\u001b[0m, in \u001b[0;36mEEGClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m features \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(features, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(features)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m(x)\n",
      "File \u001b[1;32mc:\\Users\\grego\\miniconda3\\envs\\map588\\lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EEGClassifier' object has no attribute 'f'"
     ]
    }
   ],
   "source": [
    "pretrained = torch.load('./models/classifier_pretrained_final_bis.pth')\n",
    "not_pretrained = torch.load('./models/classifier_not_pretrained.pth')\n",
    "y_true =np.array( [])\n",
    "y_pred_pretrained = np.array( [])\n",
    "y_pred_not_pretrained = np.array( [])\n",
    "for batch_x, batch_y in dataloader_val:\n",
    "    batch_x = batch_x.to(device)\n",
    "    batch_y = batch_y.float()\n",
    "    batch_y = batch_y.to(device)\n",
    "    y_pretrained = pretrained(batch_x.float())\n",
    "    y_not_pretrained = not_pretrained(batch_x.float())\n",
    "    y_true=np.concatenate((y_true, batch_y.numpy()))\n",
    "    y_pretrained = y_pretrained.detach().numpy()\n",
    "    y_not_pretrained = y_not_pretrained.detach().numpy()\n",
    "    y_pred_pretrained=np.concatenate((y_pred_pretrained, y_pretrained.flatten()))\n",
    "    y_pred_not_pretrained=np.concatenate((y_pred_not_pretrained, y_not_pretrained.flatten()))\n",
    "\n",
    "y_pred_pretrained = (y_pred_pretrained > 0.5).astype(int)\n",
    "y_pred_not_pretrained = (y_pred_not_pretrained > 0.5).astype(int)\n",
    "print('F1 score pretrained', f1_score(y_true, y_pred_pretrained))\n",
    "print('F1 score not pretrained', f1_score(y_true, y_pred_not_pretrained))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "les_pretrained= []\n",
    "les_not_pretrained = []\n",
    "for threshold in np.arange(-3, 3, 0.02):\n",
    "    y_pred_pretrained_oui = (y_pred_pretrained > threshold).astype(int)\n",
    "    y_pred_not_pretrained_oui = (y_pred_not_pretrained > threshold).astype(int)\n",
    "    les_pretrained.append(f1_score(y_true, y_pred_pretrained_oui))\n",
    "    les_not_pretrained.append(f1_score(y_true, y_pred_not_pretrained_oui))\n",
    "plt.plot( np.arange(-3, 3, 0.02),les_pretrained,  label='pretrained')\n",
    "plt.plot( np.arange(-3, 3, 0.02),   les_not_pretrained,   label='not pretrained')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('./models/extractor.pth')\n",
    "for batch_x, batch_y in dataloader_test : \n",
    "    # print(batch_x)\n",
    "    print(model(batch_x.float()))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "map588",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
